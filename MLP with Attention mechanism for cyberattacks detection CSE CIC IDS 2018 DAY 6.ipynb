{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (60.2.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.12.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Fer_U\\PycharmProjects\\CyberattacksAttention\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Fer_U\\PycharmProjects\\CyberattacksAttention\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neural_structured_learning in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from neural_structured_learning) (1.9.3)\n",
      "Requirement already satisfied: six in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from neural_structured_learning) (1.16.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from neural_structured_learning) (1.3.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from neural_structured_learning) (22.1.0)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from scipy->neural_structured_learning) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Fer_U\\PycharmProjects\\CyberattacksAttention\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install neural_structured_learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit_learn in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from scikit_learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from scikit_learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from scikit_learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\fer_u\\pycharmprojects\\cyberattacksattention\\venv\\lib\\site-packages (from scikit_learn) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Fer_U\\PycharmProjects\\CyberattacksAttention\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit_learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 4460,
     "status": "ok",
     "timestamp": 1670604423667,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "bTL3Ufo0t487",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import neural_structured_learning as nsl\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1670604426852,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "xdB9GixktNz0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('CSE_CIC_IDS_2018/02-22-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670604426856,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "856UWFEmyUms",
    "outputId": "a5a2bcaf-4a37-454e-eb77-2ffe6b9b956f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n0        22         6  22/02/2018 08:26:03       20553406            10   \n1     34989         6  22/02/2018 08:26:24            790             2   \n2       500        17  22/02/2018 08:25:10       99745913             5   \n3       500        17  22/02/2018 08:25:10       99745913             5   \n4       500        17  22/02/2018 08:24:59       89481361             6   \n\n   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n0             7             1063             1297              744   \n1             0              848                0              848   \n2             0             2500                0              500   \n3             0             2500                0              500   \n4             0             3000                0              500   \n\n   Fwd Pkt Len Min  Fwd Pkt Len Mean  Fwd Pkt Len Std  Bwd Pkt Len Max  \\\n0                0             106.3       239.357496              976   \n1                0             424.0       599.626550                0   \n2              500             500.0         0.000000                0   \n3              500             500.0         0.000000                0   \n4              500             500.0         0.000000                0   \n\n   Bwd Pkt Len Min  Bwd Pkt Len Mean  Bwd Pkt Len Std   Flow Byts/s  \\\n0                0        185.285714       363.396347  1.148228e+02   \n1                0          0.000000         0.000000  1.073418e+06   \n2                0          0.000000         0.000000  2.506368e+01   \n3                0          0.000000         0.000000  2.506368e+01   \n4                0          0.000000         0.000000  3.352654e+01   \n\n   Flow Pkts/s  Flow IAT Mean  Flow IAT Std  Flow IAT Max  Flow IAT Min  \\\n0     0.827114   1.284588e+06  4.865569e+06      19526080            12   \n1  2531.645570   7.900000e+02  0.000000e+00           790           790   \n2     0.050127   2.493648e+07  3.396804e+07      75584115       4000203   \n3     0.050127   2.493648e+07  3.396805e+07      75584130       4000189   \n4     0.067053   1.789627e+07  1.534519e+07      41990741       4000554   \n\n   Fwd IAT Tot  Fwd IAT Mean   Fwd IAT Std  Fwd IAT Max  Fwd IAT Min  \\\n0     20553406  2.283712e+06  6.467165e+06     19526080           22   \n1          790  7.900000e+02  0.000000e+00          790          790   \n2     99745913  2.493648e+07  3.396804e+07     75584115      4000203   \n3     99745913  2.493648e+07  3.396805e+07     75584130      4000189   \n4     89481361  1.789627e+07  1.534519e+07     41990741      4000554   \n\n   Bwd IAT Tot   Bwd IAT Mean    Bwd IAT Std  Bwd IAT Max  Bwd IAT Min  \\\n0       782332  130388.666667  126280.381325       245143         2639   \n1            0       0.000000       0.000000            0            0   \n2            0       0.000000       0.000000            0            0   \n3            0       0.000000       0.000000            0            0   \n4            0       0.000000       0.000000            0            0   \n\n   Fwd PSH Flags  Bwd PSH Flags  Fwd URG Flags  Bwd URG Flags  Fwd Header Len  \\\n0              0              0              0              0             212   \n1              1              0              0              0              40   \n2              0              0              0              0              40   \n3              0              0              0              0              40   \n4              0              0              0              0              48   \n\n   Bwd Header Len   Fwd Pkts/s  Bwd Pkts/s  Pkt Len Min  Pkt Len Max  \\\n0             152     0.486537    0.340576            0          976   \n1               0  2531.645570    0.000000            0          848   \n2               0     0.050127    0.000000          500          500   \n3               0     0.050127    0.000000          500          500   \n4               0     0.067053    0.000000          500          500   \n\n   Pkt Len Mean  Pkt Len Std    Pkt Len Var  FIN Flag Cnt  SYN Flag Cnt  \\\n0    131.111111   281.994971   79521.163399             0             0   \n1    565.333333   489.593028  239701.333333             0             1   \n2    500.000000     0.000000       0.000000             0             0   \n3    500.000000     0.000000       0.000000             0             0   \n4    500.000000     0.000000       0.000000             0             0   \n\n   RST Flag Cnt  PSH Flag Cnt  ACK Flag Cnt  URG Flag Cnt  CWE Flag Count  \\\n0             0             1             0             0               0   \n1             0             0             1             0               0   \n2             0             0             0             0               0   \n3             0             0             0             0               0   \n4             0             0             0             0               0   \n\n   ECE Flag Cnt  Down/Up Ratio  Pkt Size Avg  Fwd Seg Size Avg  \\\n0             0              0    138.823529             106.3   \n1             0              0    848.000000             424.0   \n2             0              0    600.000000             500.0   \n3             0              0    600.000000             500.0   \n4             0              0    583.333333             500.0   \n\n   Bwd Seg Size Avg  Fwd Byts/b Avg  Fwd Pkts/b Avg  Fwd Blk Rate Avg  \\\n0        185.285714               0               0                 0   \n1          0.000000               0               0                 0   \n2          0.000000               0               0                 0   \n3          0.000000               0               0                 0   \n4          0.000000               0               0                 0   \n\n   Bwd Byts/b Avg  Bwd Pkts/b Avg  Bwd Blk Rate Avg  Subflow Fwd Pkts  \\\n0               0               0                 0                10   \n1               0               0                 0                 2   \n2               0               0                 0                 5   \n3               0               0                 0                 5   \n4               0               0                 0                 6   \n\n   Subflow Fwd Byts  Subflow Bwd Pkts  Subflow Bwd Byts  Init Fwd Win Byts  \\\n0              1063                 7              1297              14600   \n1               848                 0                 0                234   \n2              2500                 0                 0                 -1   \n3              2500                 0                 0                 -1   \n4              3000                 0                 0                 -1   \n\n   Init Bwd Win Byts  Fwd Act Data Pkts  Fwd Seg Size Min  Active Mean  \\\n0                222                  4                20    1027304.0   \n1                 -1                  0                20          0.0   \n2                 -1                  4                 8    4000203.0   \n3                 -1                  4                 8    4000189.0   \n4                 -1                  5                 8    4000554.0   \n\n   Active Std  Active Max  Active Min     Idle Mean      Idle Std  Idle Max  \\\n0         0.0     1027304     1027304  1.952608e+07  0.000000e+00  19526080   \n1         0.0           0           0  0.000000e+00  0.000000e+00         0   \n2         0.0     4000203     4000203  3.191524e+07  3.792787e+07  75584115   \n3         0.0     4000189     4000189  3.191524e+07  3.792788e+07  75584130   \n4         0.0     4000554     4000554  2.137020e+07  1.528109e+07  41990741   \n\n   Idle Min   Label  \n0  19526080  Benign  \n1         0  Benign  \n2   7200679  Benign  \n3   7200693  Benign  \n4   7200848  Benign  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>Fwd Pkt Len Mean</th>\n      <th>Fwd Pkt Len Std</th>\n      <th>Bwd Pkt Len Max</th>\n      <th>Bwd Pkt Len Min</th>\n      <th>Bwd Pkt Len Mean</th>\n      <th>Bwd Pkt Len Std</th>\n      <th>Flow Byts/s</th>\n      <th>Flow Pkts/s</th>\n      <th>Flow IAT Mean</th>\n      <th>Flow IAT Std</th>\n      <th>Flow IAT Max</th>\n      <th>Flow IAT Min</th>\n      <th>Fwd IAT Tot</th>\n      <th>Fwd IAT Mean</th>\n      <th>Fwd IAT Std</th>\n      <th>Fwd IAT Max</th>\n      <th>Fwd IAT Min</th>\n      <th>Bwd IAT Tot</th>\n      <th>Bwd IAT Mean</th>\n      <th>Bwd IAT Std</th>\n      <th>Bwd IAT Max</th>\n      <th>Bwd IAT Min</th>\n      <th>Fwd PSH Flags</th>\n      <th>Bwd PSH Flags</th>\n      <th>Fwd URG Flags</th>\n      <th>Bwd URG Flags</th>\n      <th>Fwd Header Len</th>\n      <th>Bwd Header Len</th>\n      <th>Fwd Pkts/s</th>\n      <th>Bwd Pkts/s</th>\n      <th>Pkt Len Min</th>\n      <th>Pkt Len Max</th>\n      <th>Pkt Len Mean</th>\n      <th>Pkt Len Std</th>\n      <th>Pkt Len Var</th>\n      <th>FIN Flag Cnt</th>\n      <th>SYN Flag Cnt</th>\n      <th>RST Flag Cnt</th>\n      <th>PSH Flag Cnt</th>\n      <th>ACK Flag Cnt</th>\n      <th>URG Flag Cnt</th>\n      <th>CWE Flag Count</th>\n      <th>ECE Flag Cnt</th>\n      <th>Down/Up Ratio</th>\n      <th>Pkt Size Avg</th>\n      <th>Fwd Seg Size Avg</th>\n      <th>Bwd Seg Size Avg</th>\n      <th>Fwd Byts/b Avg</th>\n      <th>Fwd Pkts/b Avg</th>\n      <th>Fwd Blk Rate Avg</th>\n      <th>Bwd Byts/b Avg</th>\n      <th>Bwd Pkts/b Avg</th>\n      <th>Bwd Blk Rate Avg</th>\n      <th>Subflow Fwd Pkts</th>\n      <th>Subflow Fwd Byts</th>\n      <th>Subflow Bwd Pkts</th>\n      <th>Subflow Bwd Byts</th>\n      <th>Init Fwd Win Byts</th>\n      <th>Init Bwd Win Byts</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>6</td>\n      <td>22/02/2018 08:26:03</td>\n      <td>20553406</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1063</td>\n      <td>1297</td>\n      <td>744</td>\n      <td>0</td>\n      <td>106.3</td>\n      <td>239.357496</td>\n      <td>976</td>\n      <td>0</td>\n      <td>185.285714</td>\n      <td>363.396347</td>\n      <td>1.148228e+02</td>\n      <td>0.827114</td>\n      <td>1.284588e+06</td>\n      <td>4.865569e+06</td>\n      <td>19526080</td>\n      <td>12</td>\n      <td>20553406</td>\n      <td>2.283712e+06</td>\n      <td>6.467165e+06</td>\n      <td>19526080</td>\n      <td>22</td>\n      <td>782332</td>\n      <td>130388.666667</td>\n      <td>126280.381325</td>\n      <td>245143</td>\n      <td>2639</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>212</td>\n      <td>152</td>\n      <td>0.486537</td>\n      <td>0.340576</td>\n      <td>0</td>\n      <td>976</td>\n      <td>131.111111</td>\n      <td>281.994971</td>\n      <td>79521.163399</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138.823529</td>\n      <td>106.3</td>\n      <td>185.285714</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1063</td>\n      <td>7</td>\n      <td>1297</td>\n      <td>14600</td>\n      <td>222</td>\n      <td>4</td>\n      <td>20</td>\n      <td>1027304.0</td>\n      <td>0.0</td>\n      <td>1027304</td>\n      <td>1027304</td>\n      <td>1.952608e+07</td>\n      <td>0.000000e+00</td>\n      <td>19526080</td>\n      <td>19526080</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34989</td>\n      <td>6</td>\n      <td>22/02/2018 08:26:24</td>\n      <td>790</td>\n      <td>2</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>424.0</td>\n      <td>599.626550</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.073418e+06</td>\n      <td>2531.645570</td>\n      <td>7.900000e+02</td>\n      <td>0.000000e+00</td>\n      <td>790</td>\n      <td>790</td>\n      <td>790</td>\n      <td>7.900000e+02</td>\n      <td>0.000000e+00</td>\n      <td>790</td>\n      <td>790</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>2531.645570</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>848</td>\n      <td>565.333333</td>\n      <td>489.593028</td>\n      <td>239701.333333</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>848.000000</td>\n      <td>424.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>848</td>\n      <td>0</td>\n      <td>0</td>\n      <td>234</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500</td>\n      <td>17</td>\n      <td>22/02/2018 08:25:10</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.506368e+01</td>\n      <td>0.050127</td>\n      <td>2.493648e+07</td>\n      <td>3.396804e+07</td>\n      <td>75584115</td>\n      <td>4000203</td>\n      <td>99745913</td>\n      <td>2.493648e+07</td>\n      <td>3.396804e+07</td>\n      <td>75584115</td>\n      <td>4000203</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0.050127</td>\n      <td>0.000000</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>600.000000</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4000203.0</td>\n      <td>0.0</td>\n      <td>4000203</td>\n      <td>4000203</td>\n      <td>3.191524e+07</td>\n      <td>3.792787e+07</td>\n      <td>75584115</td>\n      <td>7200679</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500</td>\n      <td>17</td>\n      <td>22/02/2018 08:25:10</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.506368e+01</td>\n      <td>0.050127</td>\n      <td>2.493648e+07</td>\n      <td>3.396805e+07</td>\n      <td>75584130</td>\n      <td>4000189</td>\n      <td>99745913</td>\n      <td>2.493648e+07</td>\n      <td>3.396805e+07</td>\n      <td>75584130</td>\n      <td>4000189</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0.050127</td>\n      <td>0.000000</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>600.000000</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4000189.0</td>\n      <td>0.0</td>\n      <td>4000189</td>\n      <td>4000189</td>\n      <td>3.191524e+07</td>\n      <td>3.792788e+07</td>\n      <td>75584130</td>\n      <td>7200693</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>17</td>\n      <td>22/02/2018 08:24:59</td>\n      <td>89481361</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.352654e+01</td>\n      <td>0.067053</td>\n      <td>1.789627e+07</td>\n      <td>1.534519e+07</td>\n      <td>41990741</td>\n      <td>4000554</td>\n      <td>89481361</td>\n      <td>1.789627e+07</td>\n      <td>1.534519e+07</td>\n      <td>41990741</td>\n      <td>4000554</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0.067053</td>\n      <td>0.000000</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>583.333333</td>\n      <td>500.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>4000554.0</td>\n      <td>0.0</td>\n      <td>4000554</td>\n      <td>4000554</td>\n      <td>2.137020e+07</td>\n      <td>1.528109e+07</td>\n      <td>41990741</td>\n      <td>7200848</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1670604426856,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "mkez4dRDyZ4L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "features = len(df.columns) - 1\n",
    "\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "classes = df['Label'].nunique()\n",
    "df = df.astype(float)\n",
    "df = df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670604426857,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "6wz-53mHnm7p",
    "outputId": "fe64ae3c-8710-4fbe-fc93-ff39924faffd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n0              22         6       20553406            10             7   \n1           34989         6            790             2             0   \n2             500        17       99745913             5             0   \n3             500        17       99745913             5             0   \n4             500        17       89481361             6             0   \n...           ...       ...            ...           ...           ...   \n1042960        53        17          61898             2             2   \n1042961      1500         6       86213373             2             0   \n1042962        53        17            642             1             1   \n1042963        53        17          78472             1             1   \n1042964      3389         6        2013403             8             7   \n\n         TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n0                   1063             1297              744                0   \n1                    848                0              848                0   \n2                   2500                0              500              500   \n3                   2500                0              500              500   \n4                   3000                0              500              500   \n...                  ...              ...              ...              ...   \n1042960               78              254               39               39   \n1042961                0                0                0                0   \n1042962               39               67               39               39   \n1042963               32              121               32               32   \n1042964             1144             1581              677                0   \n\n         Fwd Pkt Len Mean  Fwd Pkt Len Std  Bwd Pkt Len Max  Bwd Pkt Len Min  \\\n0                     106              239              976                0   \n1                     424              599                0                0   \n2                     500                0                0                0   \n3                     500                0                0                0   \n4                     500                0                0                0   \n...                   ...              ...              ...              ...   \n1042960                39                0              127              127   \n1042961                 0                0                0                0   \n1042962                39                0               67               67   \n1042963                32                0              121              121   \n1042964               143              227             1173                0   \n\n         Bwd Pkt Len Mean  Bwd Pkt Len Std  Flow Byts/s  Flow Pkts/s  \\\n0                     185              363          114            0   \n1                       0                0      1073417         2531   \n2                       0                0           25            0   \n3                       0                0           25            0   \n4                       0                0           33            0   \n...                   ...              ...          ...          ...   \n1042960               127                0         5363           64   \n1042961                 0                0            0            0   \n1042962                67                0       165109         3115   \n1042963               121                0         1949           25   \n1042964               225              430         1353            7   \n\n         Flow IAT Mean  Flow IAT Std  Flow IAT Max  Flow IAT Min  Fwd IAT Tot  \\\n0              1284587       4865569      19526080            12     20553406   \n1                  790             0           790           790          790   \n2             24936478      33968040      75584115       4000203     99745913   \n3             24936478      33968050      75584130       4000189     99745913   \n4             17896272      15345187      41990741       4000554     89481361   \n...                ...           ...           ...           ...          ...   \n1042960          20632         18308         36183           454        61444   \n1042961       86213373             0      86213373      86213373     86213373   \n1042962            642             0           642           642            0   \n1042963          78472             0         78472         78472            0   \n1042964         143814        246177        953194             3      2013403   \n\n         Fwd IAT Mean  Fwd IAT Std  Fwd IAT Max  Fwd IAT Min  Bwd IAT Tot  \\\n0             2283711      6467164     19526080           22       782332   \n1                 790            0          790          790            0   \n2            24936478     33968040     75584115      4000203            0   \n3            24936478     33968050     75584130      4000189            0   \n4            17896272     15345187     41990741      4000554            0   \n...               ...          ...          ...          ...          ...   \n1042960         61444            0        61444        61444        36637   \n1042961      86213373            0     86213373     86213373            0   \n1042962             0            0            0            0            0   \n1042963             0            0            0            0            0   \n1042964        287629       397242      1177332            3      1847828   \n\n         Bwd IAT Mean  Bwd IAT Std  Bwd IAT Max  Bwd IAT Min  Fwd PSH Flags  \\\n0              130388       126280       245143         2639              0   \n1                   0            0            0            0              1   \n2                   0            0            0            0              0   \n3                   0            0            0            0              0   \n4                   0            0            0            0              0   \n...               ...          ...          ...          ...            ...   \n1042960         36637            0        36637        36637              0   \n1042961             0            0            0            0              0   \n1042962             0            0            0            0              0   \n1042963             0            0            0            0              0   \n1042964        307971       316916       953194       166195              0   \n\n         Bwd PSH Flags  Fwd URG Flags  Bwd URG Flags  Fwd Header Len  \\\n0                    0              0              0             212   \n1                    0              0              0              40   \n2                    0              0              0              40   \n3                    0              0              0              40   \n4                    0              0              0              48   \n...                ...            ...            ...             ...   \n1042960              0              0              0              16   \n1042961              0              0              0              40   \n1042962              0              0              0               8   \n1042963              0              0              0               8   \n1042964              0              0              0             172   \n\n         Bwd Header Len  Fwd Pkts/s  Bwd Pkts/s  Pkt Len Min  Pkt Len Max  \\\n0                   152           0           0            0          976   \n1                     0        2531           0            0          848   \n2                     0           0           0          500          500   \n3                     0           0           0          500          500   \n4                     0           0           0          500          500   \n...                 ...         ...         ...          ...          ...   \n1042960              16          32          32           39          127   \n1042961               0           0           0            0            0   \n1042962               8        1557        1557           39           67   \n1042963               8          12          12           32          121   \n1042964             152           3           3            0         1173   \n\n         Pkt Len Mean  Pkt Len Std  Pkt Len Var  FIN Flag Cnt  SYN Flag Cnt  \\\n0                 131          281        79521             0             0   \n1                 565          489       239701             0             1   \n2                 500            0            0             0             0   \n3                 500            0            0             0             0   \n4                 500            0            0             0             0   \n...               ...          ...          ...           ...           ...   \n1042960            74           48         2323             0             0   \n1042961             0            0            0             0             0   \n1042962            48           16          261             0             0   \n1042963            61           51         2640             0             0   \n1042964           170          319       102018             0             0   \n\n         RST Flag Cnt  PSH Flag Cnt  ACK Flag Cnt  URG Flag Cnt  \\\n0                   0             1             0             0   \n1                   0             0             1             0   \n2                   0             0             0             0   \n3                   0             0             0             0   \n4                   0             0             0             0   \n...               ...           ...           ...           ...   \n1042960             0             0             0             0   \n1042961             0             0             1             0   \n1042962             0             0             0             0   \n1042963             0             0             0             0   \n1042964             0             1             0             0   \n\n         CWE Flag Count  ECE Flag Cnt  Down/Up Ratio  Pkt Size Avg  \\\n0                     0             0              0           138   \n1                     0             0              0           848   \n2                     0             0              0           600   \n3                     0             0              0           600   \n4                     0             0              0           583   \n...                 ...           ...            ...           ...   \n1042960               0             0              1            92   \n1042961               0             0              0             0   \n1042962               0             0              1            72   \n1042963               0             0              1            92   \n1042964               0             0              0           181   \n\n         Fwd Seg Size Avg  Bwd Seg Size Avg  Fwd Byts/b Avg  Fwd Pkts/b Avg  \\\n0                     106               185               0               0   \n1                     424                 0               0               0   \n2                     500                 0               0               0   \n3                     500                 0               0               0   \n4                     500                 0               0               0   \n...                   ...               ...             ...             ...   \n1042960                39               127               0               0   \n1042961                 0                 0               0               0   \n1042962                39                67               0               0   \n1042963                32               121               0               0   \n1042964               143               225               0               0   \n\n         Fwd Blk Rate Avg  Bwd Byts/b Avg  Bwd Pkts/b Avg  Bwd Blk Rate Avg  \\\n0                       0               0               0                 0   \n1                       0               0               0                 0   \n2                       0               0               0                 0   \n3                       0               0               0                 0   \n4                       0               0               0                 0   \n...                   ...             ...             ...               ...   \n1042960                 0               0               0                 0   \n1042961                 0               0               0                 0   \n1042962                 0               0               0                 0   \n1042963                 0               0               0                 0   \n1042964                 0               0               0                 0   \n\n         Subflow Fwd Pkts  Subflow Fwd Byts  Subflow Bwd Pkts  \\\n0                      10              1063                 7   \n1                       2               848                 0   \n2                       5              2500                 0   \n3                       5              2500                 0   \n4                       6              3000                 0   \n...                   ...               ...               ...   \n1042960                 2                78                 2   \n1042961                 2                 0                 0   \n1042962                 1                39                 1   \n1042963                 1                32                 1   \n1042964                 8              1144                 7   \n\n         Subflow Bwd Byts  Init Fwd Win Byts  Init Bwd Win Byts  \\\n0                    1297              14600                222   \n1                       0                234                 -1   \n2                       0                 -1                 -1   \n3                       0                 -1                 -1   \n4                       0                 -1                 -1   \n...                   ...                ...                ...   \n1042960               254                 -1                 -1   \n1042961                 0              62568                 -1   \n1042962                67                 -1                 -1   \n1042963               121                 -1                 -1   \n1042964              1581               8192              62856   \n\n         Fwd Act Data Pkts  Fwd Seg Size Min  Active Mean  Active Std  \\\n0                        4                20      1027304           0   \n1                        0                20            0           0   \n2                        4                 8      4000203           0   \n3                        4                 8      4000189           0   \n4                        5                 8      4000554           0   \n...                    ...               ...          ...         ...   \n1042960                  1                 8            0           0   \n1042961                  0                20            0           0   \n1042962                  0                 8            0           0   \n1042963                  0                 8            0           0   \n1042964                  5                20            0           0   \n\n         Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \\\n0           1027304     1027304   19526080         0  19526080  19526080   \n1                 0           0          0         0         0         0   \n2           4000203     4000203   31915236  37927869  75584115   7200679   \n3           4000189     4000189   31915241  37927877  75584130   7200693   \n4           4000554     4000554   21370201  15281092  41990741   7200848   \n...             ...         ...        ...       ...       ...       ...   \n1042960           0           0          0         0         0         0   \n1042961           0           0   86213373         0  86213373  86213373   \n1042962           0           0          0         0         0         0   \n1042963           0           0          0         0         0         0   \n1042964           0           0          0         0         0         0   \n\n         Label  \n0            0  \n1            0  \n2            0  \n3            0  \n4            0  \n...        ...  \n1042960      0  \n1042961      0  \n1042962      0  \n1042963      0  \n1042964      0  \n\n[1042965 rows x 79 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>Fwd Pkt Len Mean</th>\n      <th>Fwd Pkt Len Std</th>\n      <th>Bwd Pkt Len Max</th>\n      <th>Bwd Pkt Len Min</th>\n      <th>Bwd Pkt Len Mean</th>\n      <th>Bwd Pkt Len Std</th>\n      <th>Flow Byts/s</th>\n      <th>Flow Pkts/s</th>\n      <th>Flow IAT Mean</th>\n      <th>Flow IAT Std</th>\n      <th>Flow IAT Max</th>\n      <th>Flow IAT Min</th>\n      <th>Fwd IAT Tot</th>\n      <th>Fwd IAT Mean</th>\n      <th>Fwd IAT Std</th>\n      <th>Fwd IAT Max</th>\n      <th>Fwd IAT Min</th>\n      <th>Bwd IAT Tot</th>\n      <th>Bwd IAT Mean</th>\n      <th>Bwd IAT Std</th>\n      <th>Bwd IAT Max</th>\n      <th>Bwd IAT Min</th>\n      <th>Fwd PSH Flags</th>\n      <th>Bwd PSH Flags</th>\n      <th>Fwd URG Flags</th>\n      <th>Bwd URG Flags</th>\n      <th>Fwd Header Len</th>\n      <th>Bwd Header Len</th>\n      <th>Fwd Pkts/s</th>\n      <th>Bwd Pkts/s</th>\n      <th>Pkt Len Min</th>\n      <th>Pkt Len Max</th>\n      <th>Pkt Len Mean</th>\n      <th>Pkt Len Std</th>\n      <th>Pkt Len Var</th>\n      <th>FIN Flag Cnt</th>\n      <th>SYN Flag Cnt</th>\n      <th>RST Flag Cnt</th>\n      <th>PSH Flag Cnt</th>\n      <th>ACK Flag Cnt</th>\n      <th>URG Flag Cnt</th>\n      <th>CWE Flag Count</th>\n      <th>ECE Flag Cnt</th>\n      <th>Down/Up Ratio</th>\n      <th>Pkt Size Avg</th>\n      <th>Fwd Seg Size Avg</th>\n      <th>Bwd Seg Size Avg</th>\n      <th>Fwd Byts/b Avg</th>\n      <th>Fwd Pkts/b Avg</th>\n      <th>Fwd Blk Rate Avg</th>\n      <th>Bwd Byts/b Avg</th>\n      <th>Bwd Pkts/b Avg</th>\n      <th>Bwd Blk Rate Avg</th>\n      <th>Subflow Fwd Pkts</th>\n      <th>Subflow Fwd Byts</th>\n      <th>Subflow Bwd Pkts</th>\n      <th>Subflow Bwd Byts</th>\n      <th>Init Fwd Win Byts</th>\n      <th>Init Bwd Win Byts</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>6</td>\n      <td>20553406</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1063</td>\n      <td>1297</td>\n      <td>744</td>\n      <td>0</td>\n      <td>106</td>\n      <td>239</td>\n      <td>976</td>\n      <td>0</td>\n      <td>185</td>\n      <td>363</td>\n      <td>114</td>\n      <td>0</td>\n      <td>1284587</td>\n      <td>4865569</td>\n      <td>19526080</td>\n      <td>12</td>\n      <td>20553406</td>\n      <td>2283711</td>\n      <td>6467164</td>\n      <td>19526080</td>\n      <td>22</td>\n      <td>782332</td>\n      <td>130388</td>\n      <td>126280</td>\n      <td>245143</td>\n      <td>2639</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>212</td>\n      <td>152</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>976</td>\n      <td>131</td>\n      <td>281</td>\n      <td>79521</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138</td>\n      <td>106</td>\n      <td>185</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1063</td>\n      <td>7</td>\n      <td>1297</td>\n      <td>14600</td>\n      <td>222</td>\n      <td>4</td>\n      <td>20</td>\n      <td>1027304</td>\n      <td>0</td>\n      <td>1027304</td>\n      <td>1027304</td>\n      <td>19526080</td>\n      <td>0</td>\n      <td>19526080</td>\n      <td>19526080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34989</td>\n      <td>6</td>\n      <td>790</td>\n      <td>2</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>424</td>\n      <td>599</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1073417</td>\n      <td>2531</td>\n      <td>790</td>\n      <td>0</td>\n      <td>790</td>\n      <td>790</td>\n      <td>790</td>\n      <td>790</td>\n      <td>0</td>\n      <td>790</td>\n      <td>790</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>2531</td>\n      <td>0</td>\n      <td>0</td>\n      <td>848</td>\n      <td>565</td>\n      <td>489</td>\n      <td>239701</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>848</td>\n      <td>424</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>848</td>\n      <td>0</td>\n      <td>0</td>\n      <td>234</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500</td>\n      <td>17</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>24936478</td>\n      <td>33968040</td>\n      <td>75584115</td>\n      <td>4000203</td>\n      <td>99745913</td>\n      <td>24936478</td>\n      <td>33968040</td>\n      <td>75584115</td>\n      <td>4000203</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>600</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4000203</td>\n      <td>0</td>\n      <td>4000203</td>\n      <td>4000203</td>\n      <td>31915236</td>\n      <td>37927869</td>\n      <td>75584115</td>\n      <td>7200679</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500</td>\n      <td>17</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>24936478</td>\n      <td>33968050</td>\n      <td>75584130</td>\n      <td>4000189</td>\n      <td>99745913</td>\n      <td>24936478</td>\n      <td>33968050</td>\n      <td>75584130</td>\n      <td>4000189</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>600</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4000189</td>\n      <td>0</td>\n      <td>4000189</td>\n      <td>4000189</td>\n      <td>31915241</td>\n      <td>37927877</td>\n      <td>75584130</td>\n      <td>7200693</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>17</td>\n      <td>89481361</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33</td>\n      <td>0</td>\n      <td>17896272</td>\n      <td>15345187</td>\n      <td>41990741</td>\n      <td>4000554</td>\n      <td>89481361</td>\n      <td>17896272</td>\n      <td>15345187</td>\n      <td>41990741</td>\n      <td>4000554</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>583</td>\n      <td>500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>4000554</td>\n      <td>0</td>\n      <td>4000554</td>\n      <td>4000554</td>\n      <td>21370201</td>\n      <td>15281092</td>\n      <td>41990741</td>\n      <td>7200848</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1042960</th>\n      <td>53</td>\n      <td>17</td>\n      <td>61898</td>\n      <td>2</td>\n      <td>2</td>\n      <td>78</td>\n      <td>254</td>\n      <td>39</td>\n      <td>39</td>\n      <td>39</td>\n      <td>0</td>\n      <td>127</td>\n      <td>127</td>\n      <td>127</td>\n      <td>0</td>\n      <td>5363</td>\n      <td>64</td>\n      <td>20632</td>\n      <td>18308</td>\n      <td>36183</td>\n      <td>454</td>\n      <td>61444</td>\n      <td>61444</td>\n      <td>0</td>\n      <td>61444</td>\n      <td>61444</td>\n      <td>36637</td>\n      <td>36637</td>\n      <td>0</td>\n      <td>36637</td>\n      <td>36637</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>16</td>\n      <td>16</td>\n      <td>32</td>\n      <td>32</td>\n      <td>39</td>\n      <td>127</td>\n      <td>74</td>\n      <td>48</td>\n      <td>2323</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>92</td>\n      <td>39</td>\n      <td>127</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>78</td>\n      <td>2</td>\n      <td>254</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1042961</th>\n      <td>1500</td>\n      <td>6</td>\n      <td>86213373</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86213373</td>\n      <td>0</td>\n      <td>86213373</td>\n      <td>86213373</td>\n      <td>86213373</td>\n      <td>86213373</td>\n      <td>0</td>\n      <td>86213373</td>\n      <td>86213373</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>62568</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86213373</td>\n      <td>0</td>\n      <td>86213373</td>\n      <td>86213373</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1042962</th>\n      <td>53</td>\n      <td>17</td>\n      <td>642</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39</td>\n      <td>67</td>\n      <td>39</td>\n      <td>39</td>\n      <td>39</td>\n      <td>0</td>\n      <td>67</td>\n      <td>67</td>\n      <td>67</td>\n      <td>0</td>\n      <td>165109</td>\n      <td>3115</td>\n      <td>642</td>\n      <td>0</td>\n      <td>642</td>\n      <td>642</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>8</td>\n      <td>1557</td>\n      <td>1557</td>\n      <td>39</td>\n      <td>67</td>\n      <td>48</td>\n      <td>16</td>\n      <td>261</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>72</td>\n      <td>39</td>\n      <td>67</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>39</td>\n      <td>1</td>\n      <td>67</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1042963</th>\n      <td>53</td>\n      <td>17</td>\n      <td>78472</td>\n      <td>1</td>\n      <td>1</td>\n      <td>32</td>\n      <td>121</td>\n      <td>32</td>\n      <td>32</td>\n      <td>32</td>\n      <td>0</td>\n      <td>121</td>\n      <td>121</td>\n      <td>121</td>\n      <td>0</td>\n      <td>1949</td>\n      <td>25</td>\n      <td>78472</td>\n      <td>0</td>\n      <td>78472</td>\n      <td>78472</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>8</td>\n      <td>12</td>\n      <td>12</td>\n      <td>32</td>\n      <td>121</td>\n      <td>61</td>\n      <td>51</td>\n      <td>2640</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>92</td>\n      <td>32</td>\n      <td>121</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>1</td>\n      <td>121</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1042964</th>\n      <td>3389</td>\n      <td>6</td>\n      <td>2013403</td>\n      <td>8</td>\n      <td>7</td>\n      <td>1144</td>\n      <td>1581</td>\n      <td>677</td>\n      <td>0</td>\n      <td>143</td>\n      <td>227</td>\n      <td>1173</td>\n      <td>0</td>\n      <td>225</td>\n      <td>430</td>\n      <td>1353</td>\n      <td>7</td>\n      <td>143814</td>\n      <td>246177</td>\n      <td>953194</td>\n      <td>3</td>\n      <td>2013403</td>\n      <td>287629</td>\n      <td>397242</td>\n      <td>1177332</td>\n      <td>3</td>\n      <td>1847828</td>\n      <td>307971</td>\n      <td>316916</td>\n      <td>953194</td>\n      <td>166195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>152</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1173</td>\n      <td>170</td>\n      <td>319</td>\n      <td>102018</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>181</td>\n      <td>143</td>\n      <td>225</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>1144</td>\n      <td>7</td>\n      <td>1581</td>\n      <td>8192</td>\n      <td>62856</td>\n      <td>5</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1042965 rows × 79 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670604427264,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "9s_HaYjkzuKk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = df.pop('Label')\n",
    "X = df\n",
    "\n",
    "normalizer_scaler = preprocessing.Normalizer()\n",
    "x_scaled = normalizer_scaler.fit_transform(X)\n",
    "X = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670604427264,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "l-9LdOome2ck",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670604427264,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "FC6lXk4Az3yB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1670604427594,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "SP8ckOayytne",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input((features,), name='feature')\n",
    "\n",
    "n1 = tf.keras.layers.Dense(16)(input)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(32)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(64)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(128)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(256)(n1)\n",
    "\n",
    "n1 = tf.keras.layers.MultiHeadAttention(num_heads=int(256/8), key_dim=256, value_dim=256, attention_axes=1)(n1, n1, n1)\n",
    "\n",
    "n1 = tf.keras.layers.Dense(256)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(128)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(64)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(32)(n1)\n",
    "n1 = tf.keras.layers.Dropout(0.25)(n1)\n",
    "n1 = tf.keras.layers.Dense(16)(n1)\n",
    "\n",
    "output = tf.keras.layers.Dense(classes, activation='softmax')(n1)\n",
    "\n",
    "model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "adv_config = nsl.configs.make_adv_reg_config(multiplier=0.2, adv_step_size=0.05)\n",
    "adv_model = nsl.keras.AdversarialRegularization(model, adv_config=adv_config)\n",
    "\n",
    "# Compile, train, and evaluate.\n",
    "adv_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1670604427855,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "-EUxz4vRy7YP",
    "outputId": "1c8c1a81-6e6b-4129-f8a2-9d22f904cb87",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " feature (InputLayer)           [(None, 78)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 16)           1264        ['feature[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 16)           0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 32)           544         ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 32)           0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64)           2112        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 64)           0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 128)          8320        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 128)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 256)          33024       ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 256)         8413440     ['dense_26[0][0]',               \n",
      " eadAttention)                                                    'dense_26[0][0]',               \n",
      "                                                                  'dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 256)          65792       ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 256)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 128)          32896       ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 128)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 64)           8256        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 64)           0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 32)           2080        ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 32)           0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 16)           528         ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 4)            68          ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,568,324\n",
      "Trainable params: 8,568,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1670604428298,
     "user": {
      "displayName": "Fernando José Rendón Segador",
      "userId": "09288482551460164544"
     },
     "user_tz": -60
    },
    "id": "844W7YPn82Gw",
    "outputId": "ea88bcec-2ee8-4229-e43f-6c1a767fdc89",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, \n",
    "                          show_shapes=True,\n",
    "                          show_dtype=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjM9Xf2Ry-lq",
    "outputId": "3f4f80c6-a5ac-4e5e-8981-be5081cd7e3c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1630/1630 [==============================] - 624s 381ms/step - loss: 0.0113 - sparse_categorical_crossentropy: 0.0094 - sparse_categorical_accuracy: 0.9990 - scaled_adversarial_loss: 0.0019 - val_loss: 0.0051 - val_sparse_categorical_crossentropy: 0.0042 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 9.0400e-04\n",
      "Epoch 2/10\n",
      "1630/1630 [==============================] - 585s 359ms/step - loss: 0.0056 - sparse_categorical_crossentropy: 0.0046 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 9.5580e-04 - val_loss: 0.0060 - val_sparse_categorical_crossentropy: 0.0049 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 0.0011\n",
      "Epoch 3/10\n",
      "1630/1630 [==============================] - 598s 367ms/step - loss: 0.3296 - sparse_categorical_crossentropy: 0.2654 - sparse_categorical_accuracy: 0.9988 - scaled_adversarial_loss: 0.0641 - val_loss: 0.0070 - val_sparse_categorical_crossentropy: 0.0058 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 0.0012\n",
      "Epoch 4/10\n",
      "1630/1630 [==============================] - 607s 373ms/step - loss: 0.0065 - sparse_categorical_crossentropy: 0.0054 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 0.0012 - val_loss: 0.0044 - val_sparse_categorical_crossentropy: 0.0036 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 7.8471e-04\n",
      "Epoch 5/10\n",
      "1630/1630 [==============================] - 600s 368ms/step - loss: 0.0051 - sparse_categorical_crossentropy: 0.0042 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 9.0956e-04 - val_loss: 0.0042 - val_sparse_categorical_crossentropy: 0.0034 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 7.4135e-04\n",
      "Epoch 6/10\n",
      "1630/1630 [==============================] - 548s 336ms/step - loss: 0.0046 - sparse_categorical_crossentropy: 0.0038 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 8.1486e-04 - val_loss: 0.0036 - val_sparse_categorical_crossentropy: 0.0029 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 6.6869e-04\n",
      "Epoch 7/10\n",
      "1630/1630 [==============================] - 547s 336ms/step - loss: 0.0045 - sparse_categorical_crossentropy: 0.0037 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 8.1309e-04 - val_loss: 0.0035 - val_sparse_categorical_crossentropy: 0.0028 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 6.4945e-04\n",
      "Epoch 8/10\n",
      "1630/1630 [==============================] - 546s 335ms/step - loss: 0.0040 - sparse_categorical_crossentropy: 0.0033 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 7.3028e-04 - val_loss: 0.0036 - val_sparse_categorical_crossentropy: 0.0029 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 6.6355e-04\n",
      "Epoch 9/10\n",
      "1630/1630 [==============================] - 548s 336ms/step - loss: 0.0040 - sparse_categorical_crossentropy: 0.0033 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 7.2725e-04 - val_loss: 0.0036 - val_sparse_categorical_crossentropy: 0.0028 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 7.0835e-04\n",
      "Epoch 10/10\n",
      "1630/1630 [==============================] - 549s 337ms/step - loss: 0.0040 - sparse_categorical_crossentropy: 0.0033 - sparse_categorical_accuracy: 0.9996 - scaled_adversarial_loss: 7.4872e-04 - val_loss: 0.0035 - val_sparse_categorical_crossentropy: 0.0029 - val_sparse_categorical_accuracy: 0.9997 - val_scaled_adversarial_loss: 6.6325e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1abb4a35bb0>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_model.fit({'feature': X_train, 'label': y_train}, epochs=10, batch_size=512, validation_data={'feature': X_test, 'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Pmt7mlJoQ-Np",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208593/208593 [==============================] - 1386s 7ms/step - loss: 0.0036 - sparse_categorical_crossentropy: 0.0029 - sparse_categorical_accuracy: 0.9997 - scaled_adversarial_loss: 6.6426e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0035500084049999714,\n 0.0028853772673755884,\n 0.9996644258499146,\n 0.0006642572116106749]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_model.evaluate({'feature':X_test, 'label':y_test}, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1l_2YpamsRqa2kvQbF6VcRh1L-ZVNw2LN",
   "authorship_tag": "ABX9TyOZ6tqQxYPy3g7WnP4d6Wrs"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}